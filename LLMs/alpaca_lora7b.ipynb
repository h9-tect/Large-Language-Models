{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2e6fa5e805d4fa88abd2f7b0a05bf2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_804462a0b38442868a9743c0ab0051b0",
              "IPY_MODEL_c44c935e4bf245f3802a477cc1649d07",
              "IPY_MODEL_90c9fe228d8d4882bc68f74e5443ecf0"
            ],
            "layout": "IPY_MODEL_b6f403934e85434993907461cb1e5d1f"
          }
        },
        "804462a0b38442868a9743c0ab0051b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01161d47a520444d9be6b09b1cf02f3c",
            "placeholder": "​",
            "style": "IPY_MODEL_42097c70b99b422692a3e24fa839bccd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c44c935e4bf245f3802a477cc1649d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28e9f2b40e8d4d2daaaac97404fadea8",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8bdc2e8cb9f44a2b11e0bba057b7c20",
            "value": 33
          }
        },
        "90c9fe228d8d4882bc68f74e5443ecf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_376b077d1ba2461f9d2dc0cef50c092f",
            "placeholder": "​",
            "style": "IPY_MODEL_b89be1818acf4aaf9e16e90b02d72455",
            "value": " 33/33 [01:09&lt;00:00,  2.24s/it]"
          }
        },
        "b6f403934e85434993907461cb1e5d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01161d47a520444d9be6b09b1cf02f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42097c70b99b422692a3e24fa839bccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28e9f2b40e8d4d2daaaac97404fadea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8bdc2e8cb9f44a2b11e0bba057b7c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "376b077d1ba2461f9d2dc0cef50c092f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89be1818acf4aaf9e16e90b02d72455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VQlevlP4q50",
        "outputId": "73477a6d-ee80-4c04-f9d0-aa99a73e5b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Setup: Required Libraries\n",
        "\n",
        "# Run the following code in Google Colab to install the necessary libraries:\n",
        "\n",
        "!pip install bitsandbytes datasets loralib sentencepiece transformers --quiet\n",
        "!pip install git+https://github.com/huggingface/peft.git --quiet\n",
        "\n",
        "# These libraries are essential for the project and provide functionalities for data processing, language modeling, and the Pretraining and Simulated Fine-Tuning (PEFT) technique.\n",
        "\n",
        "# After executing the above code, you can import and use the installed libraries in your project.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import textwrap\n",
        "\n",
        "# Third-party imports\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from transformers import LlamaTokenizer , LlamaForCausalLM, GenerationConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBNTRRdu47VJ",
        "outputId": "ebb66e56-6cd4-47ef-ca36-30bed2eea1f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-835jpd8dg70e --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the LLaMA Tokenizer and Fine-Tuned Alpaca-LoRA Model\n",
        "\n",
        "There are various options available for selecting the base and fine-tuned models. Here are some examples (not an exhaustive list) that you can choose from:\n",
        "\n",
        "- For `base_model`, you can consider the following:\n",
        "    - [decapoda-research/llama-7b-hf](https://huggingface.co/decapoda-research/llama-7b-hf)\n",
        "    - [decapoda-research/llama-13b-hf](https://huggingface.co/decapoda-research/llama-13b-hf)\n",
        "    - [decapoda-research/llama-30b-hf](https://huggingface.co/decapoda-research/llama-30b-hf)\n",
        "\n",
        "- For `finetuned_model`, you can choose from:\n",
        "    - [tloen/alpaca-lora-7b](https://huggingface.co/tloen/alpaca-lora-7b)\n",
        "    - [chansung/gpt4-alpaca-lora-7b](https://huggingface.co/chansung/gpt4-alpaca-lora-7b)\n",
        "    - [chansung/alpaca-lora-13b](https://huggingface.co/chansung/alpaca-lora-13b)\n",
        "\n",
        "*Note: The model's runtime size is dependent on the available RAM capacity.*\n",
        "\n",
        "In this section, we will use [tloen/alpaca-lora-7b](https://huggingface.co/tloen/alpaca-lora-7b) by Eric J. Wang. Once the desired model is selected, we proceed with setting up the tokenizer and model objects as follows:\n",
        "\n",
        "- The tokenizer is created using `LlamaTokenizer` from the latest `transformers` library and loaded with the LLaMA tokenizer checkpoint from the selected `base_model`.\n",
        "- The `model` is created using `LlamaForCausalLM` from the latest `transformers` library and loaded with the `base_model` checkpoint. The `load_in_8bit` parameter is set to True, which loads the model in 8-bit mode to reduce memory usage by half without any significant loss in quality. This is particularly useful when the GPU memory is limited. The `device_map` parameter is set to \"auto\" to automatically select the device (CPU or GPU) for running the model.\n"
      ],
      "metadata": {
        "id": "3SnHdBrFB-WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which model to run\n",
        "base_model = \"decapoda-research/llama-7b-hf\"  # @param [\"decapoda-research/llama-7b-hf\", \"decapoda-research/llama-13b-hf\", \"decapoda-research/llama-30b-hf\"]\n",
        "finetuned_model = \"tloen/alpaca-lora-7b\"  # @param [\"tloen/alpaca-lora-7b\", \"chansung/alpaca-lora-13b\", \"chansung/gpt4-alpaca-lora-7b\"]\n",
        "\n",
        "# Load tokenizer, base model, and fine-tuned model\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "base_model = LlamaForCausalLM.from_pretrained(base_model, load_in_8bit=True, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(base_model, finetuned_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "c2e6fa5e805d4fa88abd2f7b0a05bf2a",
            "804462a0b38442868a9743c0ab0051b0",
            "c44c935e4bf245f3802a477cc1649d07",
            "90c9fe228d8d4882bc68f74e5443ecf0",
            "b6f403934e85434993907461cb1e5d1f",
            "01161d47a520444d9be6b09b1cf02f3c",
            "42097c70b99b422692a3e24fa839bccd",
            "28e9f2b40e8d4d2daaaac97404fadea8",
            "a8bdc2e8cb9f44a2b11e0bba057b7c20",
            "376b077d1ba2461f9d2dc0cef50c092f",
            "b89be1818acf4aaf9e16e90b02d72455"
          ]
        },
        "id": "l5MvVHWa6TNW",
        "outputId": "8db41235-64ef-4492-a9f6-d8e4ff97ddd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2e6fa5e805d4fa88abd2f7b0a05bf2a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    \"\"\"\n",
        "    Generate a prompt for a given instruction and optional input.\n",
        "\n",
        "    Args:\n",
        "        instruction (str): The main instruction for the prompt.\n",
        "        input (str, optional): Additional input that provides context for the task. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        str: A prompt that includes the instruction, input (if provided), and a space for the response.\n",
        "    \"\"\"\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "        ### Instruction:\n",
        "        {instruction}\n",
        "\n",
        "        ### Input:\n",
        "        {input}\n",
        "\n",
        "        ### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "        ### Instruction:\n",
        "        {instruction}\n",
        "\n",
        "        ### Response:\"\"\"\n",
        "\n",
        "\n",
        "def alpaca_chat(context=None, temperature=0.7, top_p=0.95, repetition_penalty=1.2, max_new_tokens=512, width=100):\n",
        "    \"\"\"\n",
        "    This function prompts the user to enter a prompt and generates responses using the fine-tuned Alpaca-LoRA model.\n",
        "\n",
        "    Args:\n",
        "        context (str): Optional. A string that provides additional context to the prompt. Default is None.\n",
        "        temperature (float): Optional. A value that controls the \"creativity\" of the generated sequences. Represents the degree of randomness in the generated text. Default is 0.7.\n",
        "        top_p (float): Optional. A value that controls the \"safety\" of the generated sequences. Represents the maximum cumulative probability allowed for the generated tokens. Default is 0.95.\n",
        "        repetition_penalty (float): Optional. A value that controls the \"repetition\" of the generated sequences, penalizing the model for repeating the same tokens in a sequence. Default is 1.2.\n",
        "        max_new_tokens (int): The maximum number of new tokens that can be generated by the model in each response. Defaults to 512.\n",
        "        width (int): Optional. The maximum number of characters allowed in a single line of the generated text. Default is 100.\n",
        "\n",
        "    Example usage:\n",
        "    # Generate 5 responses using a prompt and additional context\n",
        "    alpaca_chat(context=\"I love to play video games\", n=5)\n",
        "    \"\"\"\n",
        "    input_prompt = input(\"Prompt: \")\n",
        "    print(\"-\" * 100)\n",
        "    prompt = generate_prompt(input_prompt, context)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty\n",
        "    )\n",
        "    print(\"Response:\\n\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_new_tokens\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\n",
        "            textwrap.fill(\n",
        "                output.split(\"### Response:\")[1].strip(),\n",
        "                width=width\n",
        "            )\n",
        "        )\n",
        "    print(\"-\" * 100)\n"
      ],
      "metadata": {
        "id": "CHJU_SBi6pyV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q3idDx_9e8G",
        "outputId": "55192657-d897-4cfe-eb84-4ae78bc07955"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: how can i use chatgpt\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Response:\n",
            "\n",
            "You can use ChatGPT to generate natural language dialogue between two or more participants in a\n",
            "conversation.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOy02bb2_MhH",
        "outputId": "fb6c7fb2-c13a-4ef5-d533-2851430741c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is the relativityTheory?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Response:\n",
            "\n",
            "The Relativity Theory is a branch of physics which studies how space and time are affected by\n",
            "gravity, mass and energy. It attempts to explain why objects move in curved paths around each other\n",
            "instead of moving straight lines through one another as Newton's Laws would predict.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vltrTc6Y_epw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}